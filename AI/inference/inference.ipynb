{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "102b94bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66924bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model = YOLO(\"best.pt\")\n",
    "except Exception as e:\n",
    "    print(f\"Error with loading model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84947eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import math # For math.ceil\n",
    "\n",
    "\n",
    "\n",
    "def split_image_into_tiles(\n",
    "    image_path, \n",
    "    tile_size=(320, 320),\n",
    "    padding_color=(255, 255, 255)\n",
    "):\n",
    "    try:\n",
    "        \n",
    "        img = Image.open(image_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Image file not found at {image_path}\")\n",
    "        return []\n",
    "\n",
    "    original_width, original_height = img.size\n",
    "    tile_w, tile_h = tile_size\n",
    "\n",
    "    effective_padding_color = padding_color\n",
    "\n",
    "    padded_width = math.ceil(original_width / tile_w) * tile_w\n",
    "    padded_height = math.ceil(original_height / tile_h) * tile_h\n",
    "\n",
    "    padded_img = Image.new(\n",
    "        img.mode, (padded_width, padded_height), effective_padding_color\n",
    "    )\n",
    "\n",
    "    padded_img.paste(img, (0, 0))\n",
    "\n",
    "    tiles = []\n",
    "    for y_offset in range(0, padded_height, tile_h):\n",
    "        for x_offset in range(0, padded_width, tile_w):\n",
    "            box = (x_offset, y_offset, x_offset + tile_w, y_offset + tile_h)\n",
    "            tile = padded_img.crop(box)\n",
    "            tiles.append(tile)\n",
    "\n",
    "    print(\n",
    "        f\"Original image size: {original_width}x{original_height}\"\n",
    "    )\n",
    "    if padded_width > original_width or padded_height > original_height:\n",
    "        print(\n",
    "            f\"Padded image to: {padded_width}x{padded_height} for consistent tiling.\"\n",
    "        )\n",
    "    print(f\"Split into {len(tiles)} tiles of size {tile_w}x{tile_h}.\")\n",
    "    return tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "76ea3ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image size: 1920x1080\n",
      "Padded image to: 1920x1280 for consistent tiling.\n",
      "Split into 24 tiles of size 320x320.\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "tiles = split_image_into_tiles(\"test3.jpg\")\n",
    "print(len(tiles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ac362dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fc72f270",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtiles\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "i+=1\n",
    "tiles[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "59d081e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 (no detections), 9.7ms\n",
      "Speed: 9.5ms preprocess, 9.7ms inference, 0.6ms postprocess per image at shape (1, 3, 320, 320)\n",
      "\n",
      "0: 320x320 (no detections), 7.6ms\n",
      "Speed: 0.4ms preprocess, 7.6ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 320)\n",
      "\n",
      "0: 320x320 (no detections), 8.9ms\n",
      "Speed: 0.4ms preprocess, 8.9ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 320)\n",
      "\n",
      "0: 320x320 (no detections), 7.7ms\n",
      "Speed: 0.4ms preprocess, 7.7ms inference, 0.6ms postprocess per image at shape (1, 3, 320, 320)\n",
      "\n",
      "0: 320x320 (no detections), 8.3ms\n",
      "Speed: 0.5ms preprocess, 8.3ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 320)\n",
      "\n",
      "0: 320x320 (no detections), 8.1ms\n",
      "Speed: 0.4ms preprocess, 8.1ms inference, 0.7ms postprocess per image at shape (1, 3, 320, 320)\n",
      "\n",
      "0: 320x320 (no detections), 7.8ms\n",
      "Speed: 0.5ms preprocess, 7.8ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 320)\n",
      "\n",
      "0: 320x320 (no detections), 8.5ms\n",
      "Speed: 0.6ms preprocess, 8.5ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 320)\n",
      "\n",
      "0: 320x320 (no detections), 8.5ms\n",
      "Speed: 0.4ms preprocess, 8.5ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 320)\n",
      "\n",
      "0: 320x320 (no detections), 7.9ms\n",
      "Speed: 0.4ms preprocess, 7.9ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 320)\n",
      "\n",
      "0: 320x320 (no detections), 8.5ms\n",
      "Speed: 0.9ms preprocess, 8.5ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 320)\n",
      "\n",
      "0: 320x320 (no detections), 8.9ms\n",
      "Speed: 0.4ms preprocess, 8.9ms inference, 0.6ms postprocess per image at shape (1, 3, 320, 320)\n",
      "\n",
      "0: 320x320 (no detections), 8.5ms\n",
      "Speed: 0.4ms preprocess, 8.5ms inference, 0.6ms postprocess per image at shape (1, 3, 320, 320)\n",
      "\n",
      "0: 320x320 3 surfers, 7.9ms\n",
      "Speed: 0.4ms preprocess, 7.9ms inference, 0.9ms postprocess per image at shape (1, 3, 320, 320)\n",
      "\n",
      "0: 320x320 3 surfers, 9.1ms\n",
      "Speed: 0.5ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 320)\n",
      "\n",
      "0: 320x320 1 surfer, 8.1ms\n",
      "Speed: 0.5ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 320, 320)\n",
      "\n",
      "0: 320x320 3 surfers, 8.8ms\n",
      "Speed: 0.8ms preprocess, 8.8ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 320)\n",
      "\n",
      "0: 320x320 1 surfer, 8.5ms\n",
      "Speed: 0.6ms preprocess, 8.5ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 320)\n",
      "\n",
      "0: 320x320 (no detections), 8.3ms\n",
      "Speed: 0.5ms preprocess, 8.3ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 320)\n",
      "\n",
      "0: 320x320 (no detections), 8.4ms\n",
      "Speed: 0.6ms preprocess, 8.4ms inference, 0.7ms postprocess per image at shape (1, 3, 320, 320)\n",
      "\n",
      "0: 320x320 (no detections), 8.4ms\n",
      "Speed: 0.5ms preprocess, 8.4ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 320)\n",
      "\n",
      "0: 320x320 (no detections), 8.4ms\n",
      "Speed: 0.7ms preprocess, 8.4ms inference, 0.6ms postprocess per image at shape (1, 3, 320, 320)\n",
      "\n",
      "0: 320x320 (no detections), 9.3ms\n",
      "Speed: 0.6ms preprocess, 9.3ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 320)\n",
      "\n",
      "0: 320x320 (no detections), 8.5ms\n",
      "Speed: 0.5ms preprocess, 8.5ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    }
   ],
   "source": [
    "num_surf = 0\n",
    "for img in tiles:\n",
    "    results = model(source=img, save=False, conf=0.2, device='0', imgsz=320)\n",
    "    num_surf += len(results[0].boxes.conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "44f2fc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 7 surfers, 84.2ms\n",
      "Speed: 14.2ms preprocess, 84.2ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: Failed to open Wayland display, fallback to X11. WAYLAND_DISPLAY='wayland-1' DISPLAY=':0'\n"
     ]
    }
   ],
   "source": [
    "results = model(source=tiles[72], save=False, conf=0.20, device='cpu', imgsz=320)\n",
    "results[0].show()\n",
    "len(results[0].boxes.conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7dfc79d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Boxes object with attributes:\n",
       "\n",
       "cls: tensor([])\n",
       "conf: tensor([])\n",
       "data: tensor([], size=(0, 6))\n",
       "id: None\n",
       "is_track: False\n",
       "orig_shape: (320, 320)\n",
       "shape: torch.Size([0, 6])\n",
       "xywh: tensor([], size=(0, 4))\n",
       "xywhn: tensor([], size=(0, 4))\n",
       "xyxy: tensor([], size=(0, 4))\n",
       "xyxyn: tensor([], size=(0, 4))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c88104b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results[0].boxes.conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ab3e8c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_surf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece140",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
